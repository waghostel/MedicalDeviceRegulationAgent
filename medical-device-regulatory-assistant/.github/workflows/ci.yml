name: CI/CD Pipeline - React 19 Enhanced

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run performance monitoring daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.11'
  REACT_VERSION: '19.1.0'
  PERFORMANCE_THRESHOLD_MS: 30000
  COVERAGE_THRESHOLD: 90

jobs:
  # React 19 Infrastructure Validation
  react19-compatibility:
    name: React 19 Compatibility Check
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'pnpm'
        cache-dependency-path: 'medical-device-regulatory-assistant/pnpm-lock.yaml'
    
    - name: Install pnpm
      run: npm install -g pnpm
    
    - name: Install dependencies
      working-directory: medical-device-regulatory-assistant
      run: pnpm install --frozen-lockfile
    
    - name: Verify React 19 installation
      working-directory: medical-device-regulatory-assistant
      run: |
        echo "Checking React version..."
        node -e "console.log('React version:', require('./node_modules/react/package.json').version)"
        node -e "console.log('React DOM version:', require('./node_modules/react-dom/package.json').version)"
        
        # Verify React 19 features are available
        node -e "
          const React = require('react');
          console.log('React 19 features check:');
          console.log('- React.version:', React.version);
          console.log('- Concurrent features available:', typeof React.startTransition === 'function');
          console.log('- Error boundary enhancements available:', typeof React.ErrorBoundary !== 'undefined' || 'Custom implementation required');
        "
    
    - name: Test React 19 error handling infrastructure
      working-directory: medical-device-regulatory-assistant
      run: |
        # Run specific React 19 compatibility tests
        pnpm test src/lib/testing/__tests__/react19-compatibility.test.tsx --verbose
        pnpm test src/lib/testing/__tests__/error-boundary.test.tsx --verbose
      continue-on-error: true
    
    - name: Generate React 19 compatibility report
      working-directory: medical-device-regulatory-assistant
      run: |
        mkdir -p test-reports/react19-compatibility
        echo "# React 19 Compatibility Report" > test-reports/react19-compatibility/report.md
        echo "Generated: $(date)" >> test-reports/react19-compatibility/report.md
        echo "" >> test-reports/react19-compatibility/report.md
        echo "## Version Information" >> test-reports/react19-compatibility/report.md
        echo "- React: $(node -e "console.log(require('./node_modules/react/package.json').version)")" >> test-reports/react19-compatibility/report.md
        echo "- React DOM: $(node -e "console.log(require('./node_modules/react-dom/package.json').version)")" >> test-reports/react19-compatibility/report.md
        echo "- @testing-library/react: $(node -e "console.log(require('./node_modules/@testing-library/react/package.json').version)")" >> test-reports/react19-compatibility/report.md
    
    - name: Upload React 19 compatibility report
      uses: actions/upload-artifact@v4
      with:
        name: react19-compatibility-report
        path: medical-device-regulatory-assistant/test-reports/react19-compatibility/

  frontend-tests:
    name: Frontend Tests with Performance Monitoring
    runs-on: ubuntu-latest
    needs: react19-compatibility
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'pnpm'
        cache-dependency-path: 'medical-device-regulatory-assistant/pnpm-lock.yaml'
    
    - name: Install pnpm
      run: npm install -g pnpm
    
    - name: Install dependencies
      working-directory: medical-device-regulatory-assistant
      run: pnpm install --frozen-lockfile
    
    - name: Start performance monitoring
      working-directory: medical-device-regulatory-assistant
      run: |
        echo "PERFORMANCE_START_TIME=$(date +%s)" >> $GITHUB_ENV
        echo "Starting performance monitoring at $(date)"
    
    - name: Type check with performance tracking
      working-directory: medical-device-regulatory-assistant
      run: |
        start_time=$(date +%s)
        pnpm type-check
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        echo "TYPE_CHECK_DURATION=${duration}" >> $GITHUB_ENV
        echo "Type check completed in ${duration} seconds"
    
    - name: Lint with performance tracking
      working-directory: medical-device-regulatory-assistant
      run: |
        start_time=$(date +%s)
        pnpm lint
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        echo "LINT_DURATION=${duration}" >> $GITHUB_ENV
        echo "Lint completed in ${duration} seconds"
    
    - name: Format check
      working-directory: medical-device-regulatory-assistant
      run: pnpm format:check
    
    - name: Run unit tests with coverage and performance monitoring
      working-directory: medical-device-regulatory-assistant
      run: |
        start_time=$(date +%s)
        
        # Run tests with enhanced monitoring
        pnpm test:coverage --verbose --detectOpenHandles --forceExit
        
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        echo "TEST_DURATION=${duration}" >> $GITHUB_ENV
        echo "Tests completed in ${duration} seconds"
        
        # Check if tests exceeded performance threshold
        if [ $duration -gt ${{ env.PERFORMANCE_THRESHOLD_MS }} ]; then
          echo "‚ö†Ô∏è WARNING: Test execution time (${duration}s) exceeded threshold (${{ env.PERFORMANCE_THRESHOLD_MS }}s)"
          echo "PERFORMANCE_WARNING=true" >> $GITHUB_ENV
        else
          echo "‚úÖ Test performance within acceptable limits"
          echo "PERFORMANCE_WARNING=false" >> $GITHUB_ENV
        fi
    
    - name: Generate test health report
      working-directory: medical-device-regulatory-assistant
      run: |
        mkdir -p test-reports/health
        
        # Create comprehensive test health report
        cat > test-reports/health/test-health-report.json << EOF
        {
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "ci_run_id": "${{ github.run_id }}",
          "commit_sha": "${{ github.sha }}",
          "branch": "${{ github.ref_name }}",
          "performance_metrics": {
            "type_check_duration": ${TYPE_CHECK_DURATION:-0},
            "lint_duration": ${LINT_DURATION:-0},
            "test_duration": ${TEST_DURATION:-0},
            "total_duration": $(($(date +%s) - ${PERFORMANCE_START_TIME})),
            "performance_warning": ${PERFORMANCE_WARNING:-false},
            "threshold_ms": ${{ env.PERFORMANCE_THRESHOLD_MS }}
          },
          "test_results": {
            "coverage_threshold": ${{ env.COVERAGE_THRESHOLD }},
            "react_version": "${{ env.REACT_VERSION }}",
            "node_version": "${{ env.NODE_VERSION }}"
          },
          "environment": {
            "runner_os": "${{ runner.os }}",
            "github_actor": "${{ github.actor }}",
            "event_name": "${{ github.event_name }}"
          }
        }
        EOF
        
        echo "Test health report generated"
        cat test-reports/health/test-health-report.json
    
    - name: Upload test health report
      uses: actions/upload-artifact@v4
      with:
        name: test-health-report-${{ github.run_id }}
        path: medical-device-regulatory-assistant/test-reports/health/
    
    - name: Upload frontend coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./medical-device-regulatory-assistant/coverage/lcov.info
        flags: frontend
        name: frontend-coverage
        
    - name: Run comprehensive health check
      working-directory: medical-device-regulatory-assistant
      run: |
        # Run the basic CI health check script
        node scripts/ci-health-check-basic.js
      env:
        CI: true
        CI_REPORTS_DIR: test-reports
        CI_CRITICAL_THRESHOLD: 50
        CI_WARNING_THRESHOLD: 80
        CI_FAIL_ON_HIGH_ISSUES: ${{ github.ref == 'refs/heads/main' && 'true' || 'false' }}
    
    - name: Upload health dashboard
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-health-dashboard-${{ github.run_id }}
        path: |
          medical-device-regulatory-assistant/test-reports/dashboard/
          medical-device-regulatory-assistant/test-reports/health/
    
    - name: Performance threshold check
      if: env.PERFORMANCE_WARNING == 'true'
      run: |
        echo "::warning::Test execution time exceeded performance threshold"
        echo "Consider optimizing test performance or adjusting threshold"

  backend-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
    
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v3
      with:
        path: medical-device-regulatory-assistant/backend/.venv
        key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}
    
    - name: Install dependencies
      working-directory: medical-device-regulatory-assistant/backend
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-interaction --no-root
    
    - name: Install project
      working-directory: medical-device-regulatory-assistant/backend
      run: poetry install --no-interaction
    
    - name: Run linting
      working-directory: medical-device-regulatory-assistant/backend
      run: |
        poetry run black --check .
        poetry run isort --check-only .
        poetry run flake8 .
        poetry run mypy .
    
    - name: Run unit tests with coverage
      working-directory: medical-device-regulatory-assistant/backend
      run: poetry run python -m pytest tests/ -v --cov=backend --cov-report=xml --cov-report=html --cov-fail-under=90
    
    - name: Upload backend coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./medical-device-regulatory-assistant/backend/coverage.xml
        flags: backend
        name: backend-coverage

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [frontend-tests, backend-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'pnpm'
        cache-dependency-path: 'medical-device-regulatory-assistant/pnpm-lock.yaml'
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
    
    - name: Install pnpm
      run: npm install -g pnpm
    
    - name: Install frontend dependencies
      working-directory: medical-device-regulatory-assistant
      run: pnpm install
    
    - name: Install backend dependencies
      working-directory: medical-device-regulatory-assistant/backend
      run: poetry install
    
    - name: Install Playwright
      working-directory: medical-device-regulatory-assistant
      run: pnpm exec playwright install --with-deps
    
    - name: Build frontend
      working-directory: medical-device-regulatory-assistant
      run: pnpm build
    
    - name: Start backend server
      working-directory: medical-device-regulatory-assistant/backend
      run: |
        poetry run uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10
    
    - name: Start frontend server
      working-directory: medical-device-regulatory-assistant
      run: |
        pnpm start &
        sleep 10
    
    - name: Run E2E tests
      working-directory: medical-device-regulatory-assistant
      run: pnpm test:e2e
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: playwright-report
        path: medical-device-regulatory-assistant/playwright-report/

  security-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  performance-monitoring:
    name: Performance Monitoring & Analysis
    runs-on: ubuntu-latest
    needs: [frontend-tests, backend-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'pnpm'
        cache-dependency-path: 'medical-device-regulatory-assistant/pnpm-lock.yaml'
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
    
    - name: Install pnpm
      run: npm install -g pnpm
    
    - name: Install frontend dependencies
      working-directory: medical-device-regulatory-assistant
      run: pnpm install --frozen-lockfile
    
    - name: Install backend dependencies
      working-directory: medical-device-regulatory-assistant/backend
      run: poetry install
    
    - name: Run frontend performance tests
      working-directory: medical-device-regulatory-assistant
      run: |
        echo "üöÄ Running frontend performance tests..."
        
        # Bundle size analysis
        pnpm build
        pnpm bundlesize || echo "Bundle size check completed with warnings"
        
        # Test performance monitoring
        pnpm test:performance --verbose --detectOpenHandles --forceExit
        
        # Generate performance report
        node -e "
          const fs = require('fs');
          const report = {
            timestamp: new Date().toISOString(),
            bundleSize: 'See bundlesize output',
            testPerformance: 'See test output',
            buildTime: process.env.BUILD_TIME || 'N/A'
          };
          fs.writeFileSync('performance-report.json', JSON.stringify(report, null, 2));
        "
    
    - name: Run backend performance tests
      working-directory: medical-device-regulatory-assistant/backend
      run: |
        echo "üöÄ Running backend performance tests..."
        
        # Create performance test if it doesn't exist
        mkdir -p tests/performance
        
        # Run existing performance tests or create basic ones
        if [ -d "tests/performance" ] && [ "$(ls -A tests/performance)" ]; then
          poetry run python -m pytest tests/performance/ -v --tb=short
        else
          echo "No performance tests found, creating basic performance validation..."
          poetry run python -c "
import time
import psutil
import json
from datetime import datetime

def basic_performance_check():
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss
    
    # Simulate some work
    for i in range(1000):
        _ = [j**2 for j in range(100)]
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss
    
    report = {
        'timestamp': datetime.utcnow().isoformat(),
        'execution_time': end_time - start_time,
        'memory_usage': end_memory - start_memory,
        'status': 'completed'
    }
    
    with open('backend-performance-report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f'Performance check completed in {end_time - start_time:.2f}s')
    print(f'Memory usage: {(end_memory - start_memory) / 1024 / 1024:.2f}MB')

basic_performance_check()
          "
        fi
    
    - name: Lighthouse performance audit
      working-directory: medical-device-regulatory-assistant
      run: |
        echo "üöÄ Running Lighthouse performance audit..."
        
        # Install Lighthouse if not already installed
        if ! command -v lighthouse &> /dev/null; then
          npm install -g lighthouse
        fi
        
        # Start the application in background
        pnpm build
        pnpm start &
        APP_PID=$!
        
        # Wait for app to start
        echo "Waiting for application to start..."
        timeout 60 bash -c 'until curl -f http://localhost:3000; do sleep 2; done' || {
          echo "Application failed to start within 60 seconds"
          kill $APP_PID 2>/dev/null || true
          exit 1
        }
        
        # Run Lighthouse audit
        lighthouse http://localhost:3000 \
          --output=json \
          --output-path=lighthouse-report.json \
          --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage" \
          --quiet || echo "Lighthouse completed with warnings"
        
        # Stop the application
        kill $APP_PID 2>/dev/null || true
        
        # Extract key metrics
        if [ -f "lighthouse-report.json" ]; then
          node -e "
            const report = JSON.parse(require('fs').readFileSync('lighthouse-report.json', 'utf8'));
            const metrics = report.lhr.audits;
            console.log('üìä Lighthouse Performance Metrics:');
            console.log('- Performance Score:', Math.round(report.lhr.categories.performance.score * 100));
            console.log('- First Contentful Paint:', metrics['first-contentful-paint'].displayValue);
            console.log('- Largest Contentful Paint:', metrics['largest-contentful-paint'].displayValue);
            console.log('- Cumulative Layout Shift:', metrics['cumulative-layout-shift'].displayValue);
          " || echo "Failed to parse Lighthouse report"
        fi
      continue-on-error: true
    
    - name: Generate comprehensive performance report
      working-directory: medical-device-regulatory-assistant
      run: |
        echo "üìä Generating comprehensive performance report..."
        
        node -e "
          const fs = require('fs');
          const path = require('path');
          
          const report = {
            timestamp: new Date().toISOString(),
            ci_run_id: process.env.GITHUB_RUN_ID,
            commit_sha: process.env.GITHUB_SHA,
            branch: process.env.GITHUB_REF_NAME,
            performance_data: {
              frontend: {},
              backend: {},
              lighthouse: {}
            }
          };
          
          // Load frontend performance data
          try {
            if (fs.existsSync('performance-report.json')) {
              report.performance_data.frontend = JSON.parse(fs.readFileSync('performance-report.json', 'utf8'));
            }
          } catch (e) { console.log('No frontend performance data'); }
          
          // Load backend performance data
          try {
            if (fs.existsSync('backend/backend-performance-report.json')) {
              report.performance_data.backend = JSON.parse(fs.readFileSync('backend/backend-performance-report.json', 'utf8'));
            }
          } catch (e) { console.log('No backend performance data'); }
          
          // Load Lighthouse data
          try {
            if (fs.existsSync('lighthouse-report.json')) {
              const lighthouse = JSON.parse(fs.readFileSync('lighthouse-report.json', 'utf8'));
              report.performance_data.lighthouse = {
                performance_score: Math.round(lighthouse.lhr.categories.performance.score * 100),
                fcp: lighthouse.lhr.audits['first-contentful-paint'].numericValue,
                lcp: lighthouse.lhr.audits['largest-contentful-paint'].numericValue,
                cls: lighthouse.lhr.audits['cumulative-layout-shift'].numericValue
              };
            }
          } catch (e) { console.log('No Lighthouse data'); }
          
          // Save comprehensive report
          fs.mkdirSync('test-reports/performance', { recursive: true });
          fs.writeFileSync('test-reports/performance/comprehensive-report.json', JSON.stringify(report, null, 2));
          
          console.log('üìä Performance report saved to test-reports/performance/comprehensive-report.json');
        "
    
    - name: Upload performance reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-reports-${{ github.run_id }}
        path: |
          medical-device-regulatory-assistant/test-reports/performance/
          medical-device-regulatory-assistant/performance-report.json
          medical-device-regulatory-assistant/backend/backend-performance-report.json
          medical-device-regulatory-assistant/lighthouse-report.json
    
    - name: Performance regression check
      working-directory: medical-device-regulatory-assistant
      run: |
        echo "üîç Checking for performance regressions..."
        
        # This would typically compare against baseline metrics
        # For now, we'll just validate that key metrics are within acceptable ranges
        node -e "
          const fs = require('fs');
          
          let hasRegressions = false;
          
          try {
            if (fs.existsSync('lighthouse-report.json')) {
              const lighthouse = JSON.parse(fs.readFileSync('lighthouse-report.json', 'utf8'));
              const perfScore = Math.round(lighthouse.lhr.categories.performance.score * 100);
              
              console.log('Performance Score:', perfScore);
              
              if (perfScore < 70) {
                console.log('::warning::Performance score below threshold (70)');
                hasRegressions = true;
              }
              
              const lcp = lighthouse.lhr.audits['largest-contentful-paint'].numericValue;
              if (lcp > 4000) { // 4 seconds
                console.log('::warning::Largest Contentful Paint exceeds 4s threshold');
                hasRegressions = true;
              }
            }
          } catch (e) {
            console.log('Could not analyze performance metrics');
          }
          
          if (hasRegressions && process.env.GITHUB_REF === 'refs/heads/main') {
            console.log('::error::Performance regressions detected on main branch');
            process.exit(1);
          }
        "
      continue-on-error: ${{ github.ref != 'refs/heads/main' }}